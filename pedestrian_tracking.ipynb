{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c97aa76-716b-4bfe-8f3e-4866120314e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with lxml.etree\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.tv_tensors import BoundingBoxFormat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    from lxml import etree\n",
    "    print(\"running with lxml.etree\")\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as etree\n",
    "    print(\"running with Python's xml.etree.ElementTree\")\n",
    "\n",
    "# Dataset\n",
    "# https://www.kaggle.com/datasets/karthika95/pedestrian-detection\n",
    "\n",
    "# Download latest version\n",
    "#path = kagglehub.dataset_download(\"karthika95/pedestrian-detection\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728b4a2c-1ba5-4a87-bf7e-4fbca44227ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person-like': 960, 'person': 1106}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique labels\n",
    "\n",
    "def get_all_unique_labels(annotations_file):\n",
    "    annotations_dir = Path(annotations_file)\n",
    "    labels = dict()\n",
    "    for item in annotations_dir.iterdir():\n",
    "        file_path = f\"{annotations_dir}/{item.name}\"\n",
    "        if Path(file_path).is_file():  \n",
    "            try:\n",
    "                # Parse the XML from the file\n",
    "                tree = etree.parse(file_path)\n",
    "                # Get the root element\n",
    "                root = tree.getroot()\n",
    "    \n",
    "                objects = root.findall(\"object\")\n",
    "                for obj in objects:\n",
    "                    lbl = obj.find(\"name\").text\n",
    "                    lbl_count = labels.get(lbl, 0)\n",
    "                    labels[lbl] = lbl_count + 1\n",
    "    \n",
    "            except etree.XMLSyntaxError as e:\n",
    "                print(f\"XML parsing error: {e}\")\n",
    "            except IOError as e:\n",
    "                print(f\"File error: {e}\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "train_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Train/Annotations\"\n",
    "\n",
    "get_all_unique_labels(train_annotations_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afbb9c2d-8b38-4b54-af77-d9aecccd75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "\n",
    "'''\n",
    "image: torchvision.tv_tensors.Image\n",
    "\n",
    "target: a dict containing the following fields\n",
    "    - boxes, torchvision.tv_tensors.BoundingBoxes of shape [N, 4]: the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n",
    "    - labels, integer torch.Tensor of shape [N]: the label for each bounding box. 0 represents always the background class.\n",
    "    - image_id, int: an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "    - area, float torch.Tensor of shape [N]: the area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "    - iscrowd, uint8 torch.Tensor of shape [N]: instances with iscrowd=True will be ignored during evaluation.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    # a dataset has to implement these 3 methods\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        \n",
    "        self.img_labels = self._get_labels(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        n = self.img_labels[idx][\"image_id\"]\n",
    "        num_bytes = math.ceil(n.bit_length() / 8)\n",
    "        image_name = n.to_bytes(num_bytes, 'big').decode('utf-8').split(\"/\")[1].split(\".\")[0]\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, image_name + \".jpg\")\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels[idx]\n",
    "        image = tv_tensors.Image(image)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, label = self.transforms(image, label)\n",
    "    \n",
    "        return image, label\n",
    "\n",
    "    def _get_labels(self, annotations_file):\n",
    "        labels = []\n",
    "        label_map = {\"person\": 1, \"person-like\": 2}\n",
    "        # iterate over the directory\n",
    "        annotations_dir = Path(annotations_file)\n",
    "        \n",
    "        # every image can have multiple bounding boxes\n",
    "        for item in annotations_dir.iterdir():\n",
    "            \n",
    "            file_path = f\"{annotations_dir}/{item.name}\"\n",
    "    \n",
    "            if Path(file_path).is_file():  \n",
    "                try:\n",
    "                    # Parse the XML from the file\n",
    "                    tree = etree.parse(file_path)\n",
    "                    # Get the root element\n",
    "                    root = tree.getroot()\n",
    "        \n",
    "                    # get width x height\n",
    "                    size   = root.find(\"size\")\n",
    "                    width  = int(size.find(\"width\").text)\n",
    "                    height = int(size.find(\"height\").text)\n",
    "        \n",
    "                    objects = root.findall(\"object\")\n",
    "\n",
    "                    bboxes = []\n",
    "                    lbls   = []\n",
    "                    areas  = []\n",
    "\n",
    "                    for obj in objects:\n",
    "                        # get the bounding box for each object\n",
    "                        bnd_box_xml = obj.find(\"bndbox\")\n",
    "                        x1, y1, x2, y2 = float(bnd_box_xml.find(\"xmin\").text), float(bnd_box_xml.find(\"ymin\").text), float(bnd_box_xml.find(\"xmax\").text), float(bnd_box_xml.find(\"ymax\").text)\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        areas.append(area)\n",
    "                        bboxes.append([x1, y1, x2, y2])\n",
    "                        lbls.append(label_map[obj.find(\"name\").text])\n",
    "                \n",
    "                    bboxes = tv_tensors.BoundingBoxes(\n",
    "                        bboxes,\n",
    "                        format=BoundingBoxFormat.XYXY,\n",
    "                        canvas_size=(height, width)\n",
    "                    )\n",
    "\n",
    "                    image_label = {\n",
    "                        \"boxes\": bboxes,\n",
    "                        \"labels\": torch.tensor(lbls),\n",
    "                        \"image_id\": int.from_bytes(\"/\".join(file_path.split(\"/\")[-2:]).encode('utf-8'), 'big'),\n",
    "                        \"area\": torch.tensor(areas),\n",
    "                        \"iscrowd\": torch.tensor([False for i in range(len(lbls))])\n",
    "                    }\n",
    "\n",
    "                    \n",
    "                    labels.append(image_label)\n",
    "            \n",
    "                except etree.XMLSyntaxError as e:\n",
    "                    print(f\"XML parsing error: {e}\")\n",
    "                except IOError as e:\n",
    "                    print(f\"File error: {e}\")\n",
    "            \n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f61a45a-42ec-4f54-a8e0-7cc34d04ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate training dataset\n",
    "train_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Train/Annotations\"\n",
    "train_img_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Train/JPEGImages\"\n",
    "training_dataset = MyDataset(train_annotations_dir, train_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "063edf8a-d189-4ac3-96f6-ddf3367906db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate val dataset\n",
    "val_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Val/Annotations\"\n",
    "val_img_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Val/JPEGImages\"\n",
    "val_dataset = MyDataset(val_annotations_dir, val_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6be54ea5-fe55-4245-bba3-8d3eb64f48e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Image([[[118, 118, 118,  ..., 164, 164, 164],\n",
      "        [118, 118, 119,  ..., 165, 165, 164],\n",
      "        [119, 119, 119,  ..., 166, 166, 166],\n",
      "        ...,\n",
      "        [ 82,  75,  90,  ...,  21,  22,  25],\n",
      "        [ 75,  60,  93,  ...,  22,  24,  28],\n",
      "        [ 71,  56,  99,  ...,  24,  26,  28]],\n",
      "\n",
      "       [[146, 146, 146,  ..., 179, 179, 179],\n",
      "        [146, 146, 147,  ..., 180, 180, 179],\n",
      "        [147, 147, 147,  ..., 181, 181, 181],\n",
      "        ...,\n",
      "        [ 76,  62,  67,  ...,  23,  24,  26],\n",
      "        [ 69,  47,  70,  ...,  24,  26,  29],\n",
      "        [ 65,  43,  76,  ...,  26,  28,  29]],\n",
      "\n",
      "       [[194, 194, 194,  ..., 210, 210, 210],\n",
      "        [194, 194, 195,  ..., 211, 211, 210],\n",
      "        [195, 195, 195,  ..., 212, 212, 212],\n",
      "        ...,\n",
      "        [ 44,  30,  33,  ...,   9,  10,  12],\n",
      "        [ 37,  15,  36,  ...,  10,  12,  15],\n",
      "        [ 33,  11,  42,  ...,  12,  14,  15]]], dtype=torch.uint8, ), {'boxes': BoundingBoxes([[ 35.,  21., 598., 435.]], format=BoundingBoxFormat.XYXY, canvas_size=(436, 653), clamping_mode=soft), 'labels': tensor([2]), 'image_id': 26916905455388115044085360217562572350608101956146995516354882924, 'area': tensor([233082.]), 'iscrowd': tensor([False])})\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f9ff3a8-439a-4190-88cf-bfdc9ee282fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "944"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "79d20887-7458-43e5-bc25-d7591d074938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "bfa05d86-b0ec-433d-9a87-bddd1814403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /Users/Dylan/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 160M/160M [00:05<00:00, 30.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 3  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b31bb-47ac-462c-8a40-5894f6c5c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba20a8b-e919-44a4-b29c-43172e36b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9f264c6c-fa3e-40fb-8ad5-51f637c95fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(0.2559, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0960, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0026, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0040, grad_fn=<DivBackward0>)}\n",
      "{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dataset = MyDataset(train_annotations_dir, train_img_dir, get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3683b4-f076-4192-b6c9-c90a54a0baef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/447]  eta: 1 day, 8:11:12  lr: 0.000016  loss: 0.5127 (0.5127)  loss_classifier: 0.2816 (0.2816)  loss_box_reg: 0.1736 (0.1736)  loss_objectness: 0.0071 (0.0071)  loss_rpn_box_reg: 0.0504 (0.0504)  time: 259.2225  data: 0.0058\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# train on the accelerator or on the CPU, if an accelerator is not available\n",
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device('cpu')\n",
    "\n",
    "train_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Train/Annotations\"\n",
    "train_img_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Train/JPEGImages\"\n",
    "\n",
    "val_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Val/Annotations\"\n",
    "val_img_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Val/JPEGImages\"\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 3\n",
    "# use our dataset and defined transformations\n",
    "dataset = MyDataset(train_annotations_dir, train_img_dir, get_transform(train=True))\n",
    "dataset_test = MyDataset(val_annotations_dir, val_img_dir, get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it just for 2 epochs\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bbed7-1cf6-462d-8313-110006fb7a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
