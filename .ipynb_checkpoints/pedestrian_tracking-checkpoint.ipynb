{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8c97aa76-716b-4bfe-8f3e-4866120314e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with lxml.etree\n",
      "Path to dataset files: /Users/Dylan/.cache/kagglehub/datasets/karthika95/pedestrian-detection/versions/1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import decode_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.tv_tensors import BoundingBoxFormat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    from lxml import etree\n",
    "    print(\"running with lxml.etree\")\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as etree\n",
    "    print(\"running with Python's xml.etree.ElementTree\")\n",
    "\n",
    "# Dataset\n",
    "# https://www.kaggle.com/datasets/karthika95/pedestrian-detection\n",
    "\n",
    "# Download latest version\n",
    "#path = kagglehub.dataset_download(\"karthika95/pedestrian-detection\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "afbb9c2d-8b38-4b54-af77-d9aecccd75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "\n",
    "class Label:\n",
    "    def __init__(self, filename, width, height, x1, y1, x2, y2, label_tag):\n",
    "        self.filename = filename\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.points = [(x1, y1), (x2, y2)]\n",
    "        self.label = label_tag\n",
    "\n",
    "    def __repr__(self):\n",
    "        p1, p2 = self.points\n",
    "        return f\"Name: {self.filename}\\nLabel:{self.label}\\nWidth: {self.width}\\nHeight: {self.height}\\nBnd Box: {p1},{p2}\"\n",
    "        \n",
    "\n",
    "'''\n",
    "image: torchvision.tv_tensors.Image\n",
    "\n",
    "target: a dict containing the following fields\n",
    "    - boxes, torchvision.tv_tensors.BoundingBoxes of shape [N, 4]: the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n",
    "    - labels, integer torch.Tensor of shape [N]: the label for each bounding box. 0 represents always the background class.\n",
    "    - image_id, int: an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "    - area, float torch.Tensor of shape [N]: the area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "    - iscrowd, uint8 torch.Tensor of shape [N]: instances with iscrowd=True will be ignored during evaluation.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    # a dataset has to implement these 3 methods\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        \n",
    "        self.img_labels = self._get_labels(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx].filename)\n",
    "        image = decode_image(img_path)\n",
    "        label = self.img_labels[idx]\n",
    "        image = image.numpy()\n",
    "        #label = label.numpy()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        #if self.target_transform:\n",
    "            #label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "    def _get_labels(self, annotations_file):\n",
    "        labels = []\n",
    "        # iterate over the directory\n",
    "        annotations_dir = Path(annotations_file)\n",
    "        # every image can have multiple bounding boxes\n",
    "        '''\n",
    "        label format dict(): {\n",
    "            boxes: torchvision.tv_tensors.BoundingBoxes of shape [N, 4],\n",
    "            labels: integer torch.Tensor of shape [N],\n",
    "            image_id: unique image id,\n",
    "            area: float torch.Tensor of shape [N],\n",
    "            iscrowd: uint8 torch.Tensor of shape [N] (set to False)\n",
    "        }\n",
    "        '''\n",
    "        for item in annotations_dir.iterdir():\n",
    "            \n",
    "            file_path = f\"{annotations_dir}/{item.name}\"\n",
    "    \n",
    "            if Path(file_path).is_file():  \n",
    "                try:\n",
    "                    # Parse the XML from the file\n",
    "                    tree = etree.parse(file_path)\n",
    "                    # Get the root element\n",
    "                    root = tree.getroot()\n",
    "        \n",
    "                    # get width x height\n",
    "                    size   = root.find(\"size\")\n",
    "                    width  = size.find(\"width\").text\n",
    "                    height = size.find(\"height\").text\n",
    "        \n",
    "                    objects = root.findall(\"object\")\n",
    "\n",
    "                    for obj in objects:\n",
    "                        # get the bounding box for each object\n",
    "                        bnd_box = obj.find(\"bndbox\")\n",
    "                        x_min, y_min, x_max, y_max = float(bnd_box.find(\"xmin\").text), float(bnd_box.find(\"ymin\").text), float(bnd_box.find(\"xmax\").text), float(bnd_box.find(\"ymax\").text)\n",
    "                        label_tag = obj.find(\"name\").text  \n",
    "                    # create a label                \n",
    "                    #label = Label(item.name.strip(\".xml\") + \".jpg\", width, height, x_min, y_min, x_max, y_max, label_tag)\n",
    "        \n",
    "                    #labels.append(label)\n",
    "            \n",
    "                except etree.XMLSyntaxError as e:\n",
    "                    print(f\"XML parsing error: {e}\")\n",
    "                except IOError as e:\n",
    "                    print(f\"File error: {e}\")\n",
    "            \n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6babd821-4d43-48b0-81c8-d35da281b003",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# instantiate val dataset\u001b[39;00m\n\u001b[1;32m     85\u001b[0m val_annotations_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/Dylan/Documents/ml/pedestrian_tracking/test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m get_labels(val_annotations_dir)\n",
      "Cell \u001b[0;32mIn[145], line 68\u001b[0m, in \u001b[0;36mget_labels\u001b[0;34m(annotations_file)\u001b[0m\n\u001b[1;32m     58\u001b[0m         lbls\u001b[38;5;241m.\u001b[39mappend(obj\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     60\u001b[0m     bboxes \u001b[38;5;241m=\u001b[39m tv_tensors\u001b[38;5;241m.\u001b[39mBoundingBoxes(\n\u001b[1;32m     61\u001b[0m         bboxes,\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mBoundingBoxFormat\u001b[38;5;241m.\u001b[39mXYXY,\n\u001b[1;32m     63\u001b[0m         canvas_size\u001b[38;5;241m=\u001b[39m(height, width)\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     image_label \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: bboxes,\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(lbls),\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_bytes(file_path\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbig\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(areas),\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lbls))])\n\u001b[1;32m     72\u001b[0m     }\n\u001b[1;32m     75\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(image_label)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m etree\u001b[38;5;241m.\u001b[39mXMLSyntaxError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "def get_labels(annotations_file):\n",
    "        labels = []\n",
    "        label_map = {\"person\": 0, \"person-like\": 1}\n",
    "        # iterate over the directory\n",
    "        annotations_dir = Path(annotations_file)\n",
    "        # every image can have multiple bounding boxes\n",
    "        '''\n",
    "        label format dict(): {\n",
    "            boxes: torchvision.tv_tensors.BoundingBoxes of shape [N, 4], \n",
    "            labels: integer torch.Tensor of shape [N],\n",
    "            image_id: unique image id,\n",
    "            area: float torch.Tensor of shape [N],\n",
    "            iscrowd: uint8 torch.Tensor of shape [N] (set to False)\n",
    "        }\n",
    "\n",
    "\n",
    "        # bbox example\n",
    "        canvas_size = (512, 512)\n",
    "\n",
    "        # Bounding box data in XYXY format: [[x1, y1, x2, y2], ...]\n",
    "        boxes_data = [[17, 16, 344, 495], [0, 10, 0, 10]]\n",
    "        \n",
    "        bboxes = tv_tensors.BoundingBoxes(\n",
    "            boxes_data,\n",
    "            format=BoundingBoxFormat.XYXY,\n",
    "            canvas_size=canvas_size\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        for item in annotations_dir.iterdir():\n",
    "            \n",
    "            file_path = f\"{annotations_dir}/{item.name}\"\n",
    "    \n",
    "            if Path(file_path).is_file():  \n",
    "                try:\n",
    "                    # Parse the XML from the file\n",
    "                    tree = etree.parse(file_path)\n",
    "                    # Get the root element\n",
    "                    root = tree.getroot()\n",
    "        \n",
    "                    # get width x height\n",
    "                    size   = root.find(\"size\")\n",
    "                    width  = size.find(\"width\").text\n",
    "                    height = size.find(\"height\").text\n",
    "        \n",
    "                    objects = root.findall(\"object\")\n",
    "\n",
    "                    bboxes = []\n",
    "                    lbls   = []\n",
    "                    areas  = []\n",
    "\n",
    "                    for obj in objects:\n",
    "                        # get the bounding box for each object\n",
    "                        bnd_box_xml = obj.find(\"bndbox\")\n",
    "                        x1, y1, x2, y2 = float(bnd_box_xml.find(\"xmin\").text), float(bnd_box_xml.find(\"ymin\").text), float(bnd_box_xml.find(\"xmax\").text), float(bnd_box_xml.find(\"ymax\").text)\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        areas.append(area)\n",
    "                        bboxes.append([x1, y1, x2, y2])\n",
    "                        lbls.append(label_map[obj.find(\"name\").text])\n",
    "                \n",
    "                    bboxes = tv_tensors.BoundingBoxes(\n",
    "                        bboxes,\n",
    "                        format=BoundingBoxFormat.XYXY,\n",
    "                        canvas_size=(height, width)\n",
    "                    )\n",
    "\n",
    "                    image_label = {\n",
    "                        \"boxes\": bboxes,\n",
    "                        \"labels\": torch.tensor(lbls),\n",
    "                        \"image_id\": int.from_bytes(file_path.encode('utf-8'), 'big'),\n",
    "                        \"area\": torch.tensor(areas),\n",
    "                        \"iscrowd\": torch.tensor([False for i in range(len(lbls))])\n",
    "                    }\n",
    "\n",
    "                    \n",
    "                    labels.append(image_label)\n",
    "            \n",
    "                except etree.XMLSyntaxError as e:\n",
    "                    print(f\"XML parsing error: {e}\")\n",
    "                except IOError as e:\n",
    "                    print(f\"File error: {e}\")\n",
    "            \n",
    "        return labels\n",
    "\n",
    "# instantiate val dataset\n",
    "val_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/test\"\n",
    "get_labels(val_annotations_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f61a45a-42ec-4f54-a8e0-7cc34d04ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate training dataset\n",
    "train_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Train/Annotations\"\n",
    "train_img_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Train/JPEGImages\"\n",
    "training_dataset = MyDataset(train_annotations_dir, train_img_dir, transform=ToTensor(), target_transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "063edf8a-d189-4ac3-96f6-ddf3367906db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate val dataset\n",
    "val_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Val/Annotations\"\n",
    "val_img_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Val/JPEGImages\"\n",
    "val_dataset = MyDataset(val_annotations_dir, val_img_dir, transform=ToTensor(), target_transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6be54ea5-fe55-4245-bba3-8d3eb64f48e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([653, 3, 436])\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f9ff3a8-439a-4190-88cf-bfdc9ee282fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "944"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "79d20887-7458-43e5-bc25-d7591d074938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0992fc4c-e701-44d5-a3f7-4963f94a2696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bfa05d86-b0ec-433d-9a87-bddd1814403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to start with a basic NN (no Convolutional layers)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570bfe7-07e8-43c1-9b23-1363a44d2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "394b31bb-47ac-462c-8a40-5894f6c5c72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person-like': 960, 'person': 1106}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique labels\n",
    "\n",
    "def get_all_unique_labels(annotations_file):\n",
    "    annotations_dir = Path(annotations_file)\n",
    "    labels = dict()\n",
    "    for item in annotations_dir.iterdir():\n",
    "        file_path = f\"{annotations_dir}/{item.name}\"\n",
    "        if Path(file_path).is_file():  \n",
    "            try:\n",
    "                # Parse the XML from the file\n",
    "                tree = etree.parse(file_path)\n",
    "                # Get the root element\n",
    "                root = tree.getroot()\n",
    "    \n",
    "                objects = root.findall(\"object\")\n",
    "                for obj in objects:\n",
    "                    lbl = obj.find(\"name\").text\n",
    "                    lbl_count = labels.get(lbl, 0)\n",
    "                    labels[lbl] = lbl_count + 1\n",
    "    \n",
    "               \n",
    "    \n",
    "            except etree.XMLSyntaxError as e:\n",
    "                print(f\"XML parsing error: {e}\")\n",
    "            except IOError as e:\n",
    "                print(f\"File error: {e}\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "train_annotations_dir = \"/Users/Dylan/Documents/ml/pedestrian_tracking/dataset/Train/Annotations\"\n",
    "\n",
    "get_all_unique_labels(train_annotations_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba20a8b-e919-44a4-b29c-43172e36b38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
